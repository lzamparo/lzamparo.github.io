<!DOCTYPE html>
<html lang="en">

  <head>
      <title>Lee Zamparo</title>
      <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700' rel='stylesheet' type='text/css' />
      <link href='http://fonts.googleapis.com/css?family=Merriweather:300' rel='stylesheet' type='text/css'/>
      <link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:200,400,700' rel='stylesheet' type='text/css'/>
      <link rel="stylesheet" type="text/css" href="http://lzamparo.github.io/theme/css/icons.css"/>
      <link rel="stylesheet" type="text/css" href="http://lzamparo.github.io/theme/css/styles.css"/>
      <meta charset="utf-8" />
  </head>

  <body id="index">
    <!-- header -->
    <header class="siteheader">
      <!-- site image -->
        <div class= "siteimage">
          <a class="nodec" href=https://en.gravatar.com/userimage/6827348/8ddec1ea955824dea50c908ad3154623.png?size=200>
            <img width="200" height="200" src=https://en.gravatar.com/userimage/6827348/8ddec1ea955824dea50c908ad3154623.png?size=200>
          </a>
        </div>

      <div class = "sitebanner">
        <h1><a class="sitetitle nodec" href="http://lzamparo.github.io">Lee Zamparo</a></h1>
        <h3 class ="sitesubtitle">1. Get the data 2. Learn a model 3. ??? 4. Knowledge</h3>
        <!-- nav -->
        <nav class="menu">
          <ul>
            <!-- menu items-->
              <li><a class="nodec" href="/">Blog</a></li>
              <li><a class="nodec" href="/pdfs/lee_cv.pdf">CV</a></li>
            <!--pages-->
            <!-- services icons -->
              <li><a class="nodec icon-github" href="https://github.com/lzamparo"></a></li>
              <li><a class="nodec icon-twitter" href="https://twitter.com/lzamparo"></a></li>
          </ul>
        </nav>
      </div> <!-- sitebanner -->
    </header>

    <!-- content -->

<section class="content">

  <h3 class="posttitle">
    <a class="nodec" href="/bayesian-convolutional-neural-networks-with-bernoulli-approximate-variational-inference.html" rel="bookmark" title="Permalink to Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference">
      Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference
    </a>
  </h3>

  <div class="postinfo">
    <p class="published" title="2016-12-08T23:45:00-05:00">
      Thu 08 December 2016
    </p>

  </div><!-- .postinfo -->

  <div class="article">
    <p>After a long hiatus, I’m going to write about another paper <a href="http://mlg.eng.cam.ac.uk/yarin/PDFs/NIPS_2015_bayesian_convnets.pdf">I read recently</a>, that is changing the way I think about using deep networks for biological sequence problems.</p>
<h2>Background</h2>
<p>Anyone that’s working in applied machine learning these days is familiar with the idea of convolutional neural networks (abbr. convent).  They have a long history of success in all sorts of supervised learning tasks (usually object recognition in images), but have since been extended to almost any domain you can properly define a convolution (or more often cross-correlation, see this excellent chapter from <a href="http://www.deeplearningbook.org/contents/convnets.html">the deep learning textbook</a> for more details).  The key idea behind a convolutional network, at least in the context of a unit that is composed to build a deeper model, is that if can be understood as a sparse form of a fully connected layer with a specific way of tying together the values of certain parameters.  This reduction in the ratio of free parameters to training data means we should be able to get better estimates for the model parameters without requiring anything more than the tools used for training regular feed forward nets.  </p>
<p>A typical contemporary convnet, however, will employ very many local receptive fields (also known as filters), which act as feature detectors, possibly in a larger network.  They employ many, many layers, in pursuit of better discriminative performance, which eats away at any previously realized gains in reducing the number of parameters.  All this is to say that they may overfit in situations where training data is scarce.  <a href="http://mlg.eng.cam.ac.uk/yarin/PDFs/NIPS_2015_bayesian_convnets.pdf">This paper</a>, by Yarin Gal and Zoubin Ghahramani, examines how placing a distribution over the kernels that parameterize the CNNs, they  in a Bayesian framework can control overfitting naturally, without either incurring much more computational overhead or significantly altering the form of the network. </p>
<h2>Summary</h2>
<p>The authors pick up on a previous line of work showing that dropout in neural networks can be interpreted as an approximation to GPs.  In this work, they go on to show that networks trained using <a href="http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">dropout</a> can be interpreted as variational inference in Bayesian neural networks, using Bernoulli approximating distributions.  </p>
<p>Dropout, usually understood in the context of model averaging, is instead cast here as approximate variational inference in a Bayesian neural network model.  What this means I’ll establish below.  </p>
<h2>Bayesian Neural Nets</h2>
<p>The defining feature of a Bayesian neural network is the treatment of model weights as random variables.  The authors use a Gaussian distribution : </p>
<div class="math">$$\mathbf{W}_i \backsim \mathcal{N}(0,\mathbf{I})$$</div>
<p>
Leaving out the bias vectors for simplicity and assuming a multi-class likelihood, the output of the network given input <span class="math">\(x\)</span> is: 
</p>
<div class="math">$$ \hat{f}(x, (\mathbf{W}_i)_{i=1}^{L}) $$</div>
<p>
which itself is a random variable.  If we abbreviate the output as <span class="math">\(\hat{f} := \hat{f}(\mathbf{x}, (\mathbf{W_i}_{i=i}^{L}))\)</span>, where I indexes each of the <span class="math">\(L\)</span> layers in the model, then the likelihood is: 
</p>
<div class="math">$$ p(y | \mathbf{x}, (\mathbf{W}_i)_{i=1}^{L}) = \text{Cat}\left( \frac{exp(\hat{f})}{\sum\limits_{d’}exp(\hat{f_{d’}})} \right)$$</div>
<p>In contrast with most neural nets, where the goal is to optimize the values of the model parameters to minimize some loss or objective function, the goal of a Bayesian NN is to estimate the posterior distribution over the model parameters: <span class="math">\(P((\mathbf{W}_i)_{i=1}^{L}|\mathbf{X},\mathbf{Y})\)</span>.  Exact calculation of the posterior is intractable in general, but can be estimated by either MCMC or variational inference.  The authors here choose the latter, placing an approximating variational distribution over the weights <span class="math">\(q((\mathbf{W}_i))\)</span> (hereafter, this is how I’ll refer to the weights of all layers in the network), and minimizing the KL divergence between <span class="math">\(q\)</span> and the true posterior:
</p>
<div class="math">$$ KL\left( q((\mathbf{W}_i))||p((\mathbf{W}_i)|\mathbf{X},\mathbf{Y}) \right) $$</div>
<h2>Bernoulli variational approximation, and MC dropout</h2>
<p>The authors define their approximating variational distribution <span class="math">\(q(\mathbf{W}_i)\)</span> for each layer <span class="math">\(i\)</span> as 
</p>
<div class="math">\begin{align}
\mathbf{W}_i &amp;= \mathbf{M}_i \cdot \text{diag}\left( \left[ \mathbf{z}_{i,j} \right]_{j=1}^{K_i} \right) \\
\mathbf{z}_{i,j} &amp;\backsim \text{Bernoulli}(p_i)\; \text{for} i = 1, \dots, L,\; j = 1, \dots, K_{i-1}
\end{align}</div>
<p>The <span class="math">\(\mathbf{z}_{i,j}\)</span> are Bernoulli distributed random variables, and the <span class="math">\(M_i\)</span> are variational parameters.  So, once the posterior is approximated using VI, you use the model to generate predictions for new data <span class="math">\(\mathbf{x}^{*}\)</span> by replacing the (intractable) true posterior with the approximate one:</p>
<div class="math">\begin{align}
p(y^{*} | \mathbf{x}^{*},\mathbf{X},\mathbf{Y}) &amp;\approx \int p\left(y^{*}|\mathbf{x}^{*},(\mathbf{W}_i)\right)q\left((\mathbf{W}_i) \right) \approx \frac{1}{T}\sum\limits_{t=1}^{T}p\left( y^{*}|\mathbf{x}^{*},(\mathbf{W}_i)_t \right)
\end{align}</div>
<p>To clarify, in the above approximation, you draw <span class="math">\(t\)</span> different <span class="math">\((\mathbf{W}_i)\)</span> from <span class="math">\((\mathbf{W}_i)_t \backsim q\left( (\mathbf{W}_i)\right)\)</span>; the posterior is approximated via MC integration.  </p>
<p>This is the coolest part of the paper.  It says that once you find a good <span class="math">\(q\left((\mathbf{W})_i \right)\)</span>, you can get fully Bayesian predictions for new data by simply averaging together several runs of the new input through your model, each time drawing a new set of realized weights from <span class="math">\(q\)</span> along with dropout masks <span class="math">\(\mathbf{z}\)</span>.  They call this MC dropout.  </p>
<h2>My two cents</h2>
<p>The paper continues on to test their new proposed dropout (which can apply equally well to weights representing kernels in a convolutional NN as well as fully connected layers) against a set of comparative networks on both MNIST and CIFAR10, measuring the resistance to overfitting conferred by MC dropout on standard models (lenet), versus conventional dropout.  I won’t reproduce the figures here, but their experiments show a small but significant effect.</p>
<p>A fine side-point to take note of is the methodology used to measure overfitting: sub-divide your labeled training data, train on a subset, and measure how quickly the model begins to overfit.  Simple, and effective.  </p>
<p>So, to sum up, I really like this paper.  For very little extra computational effort, you can turn your convnet into a Bayesian convnet.  This gives you the dual benefits of better protection against over-fitting, as well as uncertainty estimates in your predictions, two crucial qualities that are lacking in most deep learning models for biological sequence data.  I’m excited to try it out!</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div><!-- .content -->

</section>


    <!-- footer -->
    <footer>
      <p>
        © Lee Zamparo, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a> with
        <a href="http://github.com/porterjamesj/crowsfoot">crowsfoot</a> theme.
      </p>
    </footer>
  </body>
</html>